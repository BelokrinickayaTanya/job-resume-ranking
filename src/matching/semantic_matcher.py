"""
–ú–æ–¥—É–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞ (5% –≤–µ—Å–∞ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å–∫–æ—Ä–µ)
"""
from typing import Dict, List, Optional, Union
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re


class SemanticMatcher:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç—á–∏–Ω–≥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º sentence-transformers
    
    –í–µ—Å –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å–∫–æ—Ä–µ: 5%
    """
    
    # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç –ª–µ–≥–∫–∏—Ö –∫ —Ç—è–∂–µ–ª—ã–º
    AVAILABLE_MODELS = {
        'mini': 'all-MiniLM-L6-v2',        # –ë—ã—Å—Ç—Ä–∞—è, 384-dim
        'base': 'all-mpnet-base-v2',        # –¢–æ—á–Ω–∞—è, 768-dim
        'large': 'all-roberta-large-v1',    # –û—á–µ–Ω—å —Ç–æ—á–Ω–∞—è, 1024-dim
        'msmarco': 'msmarco-distilbert-base-v4',  # –î–ª—è IR –∑–∞–¥–∞—á
    }
    
    def __init__(self, model_name: str = 'mini', device: str = 'cpu'):
        """
        Args:
            model_name: –ö–ª—é—á –∏–∑ AVAILABLE_MODELS –∏–ª–∏ –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            device: 'cpu' –∏–ª–∏ 'cuda'
        """
        self.model_name = self.AVAILABLE_MODELS.get(model_name, model_name)
        self.device = device
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ sentence-transformers"""
        try:
            self.model = SentenceTransformer(self.model_name, device=self.device)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {self.model_name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print(f"üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å fallback –º–æ–¥–µ–ª—å: all-MiniLM-L6-v2")
            self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
    
    def encode(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text = self._prepare_text(text)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        embedding = self.model.encode(text, normalize_embeddings=True)
        
        return embedding
    
    def encode_batch(self, texts: List[str]) -> np.ndarray:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        texts = [self._prepare_text(t) for t in texts]
        embeddings = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=False)
        
        return embeddings
    
    def calculate_similarity(
        self, 
        resume_text: str, 
        vacancy_text: str,
        use_chunks: bool = True
    ) -> Dict:
        """
        –†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–µ–π
        
        Args:
            resume_text: –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            vacancy_text: –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
            use_chunks: –†–∞–∑–±–∏–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
        
        Returns:
            {
                'score': 0-100,
                'similarity': float,
                'method': str
            }
        """
        if use_chunks and len(resume_text) > 1000:
            # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ —Å —á–∞–Ω–∫–∞–º–∏
            return self._calculate_similarity_chunked(resume_text, vacancy_text)
        else:
            # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä—è–º–æ–π –º–µ—Ç–æ–¥
            return self._calculate_similarity_direct(resume_text, vacancy_text)
    
    def _calculate_similarity_direct(self, resume_text: str, vacancy_text: str) -> Dict:
        """–ü—Ä—è–º–æ–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        resume_emb = self.encode(resume_text)
        vacancy_emb = self.encode(vacancy_text)
        
        similarity = self._cosine_similarity(resume_emb, vacancy_emb)
        score = similarity * 100
        
        return {
            'score': round(score, 1),
            'similarity': round(similarity, 4),
            'method': 'direct'
        }
    
    def _calculate_similarity_chunked(self, resume_text: str, vacancy_text: str) -> Dict:
        """
        –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º —Ä–µ–∑—é–º–µ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞–Ω–∫–∏
        
        –ú–µ—Ç–æ–¥: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ä–µ–∑—é–º–µ –Ω–∞ —á–∞–Ω–∫–∏ (—Å–µ–∫—Ü–∏–∏)
        chunks = self._split_into_chunks(resume_text)
        
        if not chunks:
            return self._calculate_similarity_direct(resume_text[:1000], vacancy_text)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–∏
        vacancy_emb = self.encode(vacancy_text)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
        chunk_embs = self.encode_batch(chunks)
        
        # –°—Ö–æ–∂–µ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π
        similarities = []
        for chunk_emb in chunk_embs:
            sim = self._cosine_similarity(chunk_emb, vacancy_emb)
            similarities.append(sim)
        
        # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å–µ–∫—Ü–∏—è)
        max_similarity = max(similarities) if similarities else 0
        # –ò —Å—Ä–µ–¥–Ω—é—é (–æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        avg_similarity = np.mean(similarities) if similarities else 0
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º: 70% –º–∞–∫—Å + 30% —Å—Ä–µ–¥–Ω–µ–µ
        similarity = max_similarity * 0.7 + avg_similarity * 0.3
        score = similarity * 100
        
        return {
            'score': round(score, 1),
            'similarity': round(similarity, 4),
            'max_similarity': round(max_similarity, 4),
            'avg_similarity': round(avg_similarity, 4),
            'method': 'chunked',
            'num_chunks': len(chunks)
        }
    
    def _prepare_text(self, text: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        if not text:
            return ""
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (–º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –ª–∏–º–∏—Ç 512 —Ç–æ–∫–µ–Ω–æ–≤)
        # –ü—Ä–∏–º–µ—Ä–Ω–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤
        if len(text) > 2000:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü (–≥–¥–µ –æ–±—ã—á–Ω–æ –∫–ª—é—á–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
            text = text[:1000] + " " + text[-1000:]
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞–Ω–∫–∏ (–ø–æ —Å–µ–∫—Ü–∏—è–º)"""
        chunks = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ —Ä–µ–∑—é–º–µ
        section_patterns = [
            r'(?:^|\n)(?:experience|work history|employment)[^\n]*(?:\n)(.*?)(?=\n\s*\n|\n\s*[A-Z]|\Z)',
            r'(?:^|\n)(?:education|academic)[^\n]*(?:\n)(.*?)(?=\n\s*\n|\n\s*[A-Z]|\Z)',
            r'(?:^|\n)(?:skills|technologies|competencies)[^\n]*(?:\n)(.*?)(?=\n\s*\n|\n\s*[A-Z]|\Z)',
            r'(?:^|\n)(?:projects?|portfolio)[^\n]*(?:\n)(.*?)(?=\n\s*\n|\n\s*[A-Z]|\Z)',
            r'(?:^|\n)(?:certifications?|licenses)[^\n]*(?:\n)(.*?)(?=\n\s*\n|\n\s*[A-Z]|\Z)',
            r'(?:^|\n)(?:languages)[^\n]*(?:\n)(.*?)(?=\n\s*\n|\n\s*[A-Z]|\Z)'
        ]
        
        for pattern in section_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match and match.group(1).strip():
                chunk = match.group(1).strip()
                if len(chunk) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Å–µ–∫—Ü–∏–∏
                    chunks.append(chunk)
        
        # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        if not chunks:
            paragraphs = text.split('\n\n')
            chunks = [p.strip() for p in paragraphs if len(p.strip()) > 100]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        return chunks[:10]
    
    @staticmethod
    def _cosine_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        return float(np.dot(emb1, emb2))
    
    def find_most_similar_vacancy(self, resume_text: str, vacancy_texts: List[str]) -> Dict:
        """
        –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ —Å–ø–∏—Å–∫–∞
        
        Returns:
            {
                'best_index': int,
                'best_score': float,
                'scores': List[float]
            }
        """
        resume_emb = self.encode(resume_text)
        vacancy_embs = self.encode_batch(vacancy_texts)
        
        scores = []
        for vac_emb in vacancy_embs:
            sim = self._cosine_similarity(resume_emb, vac_emb)
            scores.append(sim * 100)
        
        best_idx = np.argmax(scores)
        
        return {
            'best_index': int(best_idx),
            'best_score': round(scores[best_idx], 1),
            'scores': [round(s, 1) for s in scores]
        }