"""
–ú–æ–¥—É–ª—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ —Å –≤–µ—Å–∞–º–∏ –∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
- Keyword matching: 80%
- TF-IDF matching: 15%
- Semantic matching: 5%
"""
from typing import Dict, List, Optional, Tuple
import numpy as np
from dataclasses import dataclass, field
from datetime import datetime

from .keyword_matcher import KeywordMatcher
from .tfidf_matcher import AdaptiveTfidfMatcher
from .semantic_matcher import SemanticMatcher


@dataclass
class MatchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–∏"""
    cv_id: int
    vacancy_id: int
    vacancy_title: str
    
    # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Å–∫–æ—Ä—ã
    keyword_score: float = 0.0
    tfidf_score: float = 0.0
    semantic_score: float = 0.0
    
    # –û–±—â–∏–π —Å–∫–æ—Ä
    total_score: float = 0.0
    
    # –î–µ—Ç–∞–ª–∏
    matched_skills: List[str] = field(default_factory=list)
    missing_skills: List[str] = field(default_factory=list)
    match_details: Dict = field(default_factory=dict)
    
    # –í—Ä–µ–º—è —Ä–∞—Å—á–µ—Ç–∞
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


class UnifiedScorer:
    """
    –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–∫–æ—Ä—Ä–µ—Ä —Å –≤–µ—Å–∞–º–∏ –∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
    Keyword: 60% - —Ç–æ—á–Ω–æ–µ –∏ —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤
    TF-IDF: 25% - –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
    Semantic: 15% - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤
    """
    
    # –í–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
    WEIGHTS = {
        'keyword': 0.60,  # 60%
        'tfidf': 0.25,    # 25%
        'semantic': 0.15  # 15%
    }
    
    def __init__(
        self,
        use_adaptive_tfidf: bool = True,
        semantic_model: str = 'mini',
        device: str = 'cpu'
    ):
        """
        Args:
            use_adaptive_tfidf: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AdaptiveTfidfMatcher —Å —Ç–µ—Ö-–±—É—Å—Ç–æ–º
            semantic_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞
            device: 'cpu' –∏–ª–∏ 'cuda'
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—á–µ—Ä–æ–≤
        self.keyword_matcher = KeywordMatcher(fuzzy_threshold=0.85)
        
        if use_adaptive_tfidf:
            self.tfidf_matcher = AdaptiveTfidfMatcher(max_features=2000, ngram_range=(1, 2))
        else:
            self.tfidf_matcher = AdaptiveTfidfMatcher(max_features=2000, ngram_range=(1, 2))
        
        self.semantic_matcher = SemanticMatcher(model_name=semantic_model, device=device)
        
        # –ö—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_cache = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_matches': 0,
            'avg_keyword_score': 0,
            'avg_tfidf_score': 0,
            'avg_semantic_score': 0,
            'avg_total_score': 0
        }
    
    def calculate_score(
        self,
        cv_id: int,
        cv_text: str,
        cv_skills: List[str],
        vacancy_id: int,
        vacancy_title: str,
        vacancy_text: str,
        vacancy_skills: List[str],
        cv_experience: Optional[float] = None,
        vacancy_required_years: Optional[int] = None,
        corpus: Optional[List[str]] = None
    ) -> MatchResult:
        """
        –ü–æ–ª–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—ã —Ä–µ–∑—é–º–µ-–≤–∞–∫–∞–Ω—Å–∏—è
        """
        # 1. Keyword matching (80%)
        keyword_result = self.keyword_matcher.calculate_match_score(
            resume_skills=cv_skills,
            vacancy_skills=vacancy_skills,
            vacancy_text=vacancy_text,
            cv_experience=cv_experience,  
            vacancy_required_years=vacancy_required_years
        )
        keyword_score = keyword_result['score']
        
        # 2. TF-IDF matching (15%)
        tfidf_result = self.tfidf_matcher.calculate_similarity(
            resume_text=cv_text,
            vacancy_text=vacancy_text,
            corpus=corpus
        )
        tfidf_score = tfidf_result['score']
        
        # 3. Semantic matching (5%)
        semantic_result = self.semantic_matcher.calculate_similarity(
            resume_text=cv_text,
            vacancy_text=vacancy_text,
            use_chunks=True
        )
        semantic_score = semantic_result['score']
        
        # 4. –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        total_score = (
            self.WEIGHTS['keyword'] * keyword_score +
            self.WEIGHTS['tfidf'] * tfidf_score +
            self.WEIGHTS['semantic'] * semantic_score
        )
        
        # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 1 –∑–Ω–∞–∫–∞
        total_score = round(total_score, 1)
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = MatchResult(
            cv_id=cv_id,
            vacancy_id=vacancy_id,
            vacancy_title=vacancy_title,
            keyword_score=round(keyword_score, 1),
            tfidf_score=round(tfidf_score, 1),
            semantic_score=round(semantic_score, 1),
            total_score=total_score,
            matched_skills=keyword_result['matched'][:20],
            missing_skills=keyword_result['missing'][:20],
            match_details={
                'keyword': keyword_result,
                'tfidf': tfidf_result,
                'semantic': semantic_result
            }
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._update_stats(result)
        
        return result
    
    def calculate_batch(
        self,
        cv_dict: Dict[int, Dict],
        vacancy_dict: Dict[int, Dict],
        corpus: Optional[List[str]] = None
    ) -> List[MatchResult]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä —Ä–µ–∑—é–º–µ-–≤–∞–∫–∞–Ω—Å–∏—è
        """
        results = []
        total_pairs = len(cv_dict) * len(vacancy_dict)
        
        print(f"üîÑ –†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∏–Ω–≥–∞ –¥–ª—è {len(cv_dict)} —Ä–µ–∑—é–º–µ –∏ {len(vacancy_dict)} –≤–∞–∫–∞–Ω—Å–∏–π...")
        print(f"üìä –í—Å–µ–≥–æ –ø–∞—Ä: {total_pairs}")
        print(f"‚öñÔ∏è –í–µ—Å–∞: Keyword={self.WEIGHTS['keyword']*100}%, TF-IDF={self.WEIGHTS['tfidf']*100}%, Semantic={self.WEIGHTS['semantic']*100}%")
        print("-" * 60)
        
        processed = 0
        for cv_id, cv_data in cv_dict.items():
            for vac_id, vac_data in vacancy_dict.items():
                result = self.calculate_score(
                    cv_id=cv_id,
                    cv_text=cv_data['text'],
                    cv_skills=cv_data['skills'],
                    vacancy_id=vac_id,
                    vacancy_title=vac_data['title'],
                    vacancy_text=vac_data['description'],
                    vacancy_skills=vac_data['skills'],
                    corpus=corpus
                )
                results.append(result)
                
                processed += 1
                if processed % 50 == 0:
                    print(f"  ‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{total_pairs} –ø–∞—Ä...")
        
        print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{total_pairs} –ø–∞—Ä")
        print("-" * 60)
        
        return results
    
    def rank_vacancies_for_cv(
        self,
        cv_id: int,
        cv_data: Dict,
        vacancies: Dict[int, Dict],
        corpus: Optional[List[str]] = None
    ) -> List[MatchResult]:
        """
        –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
        """
        results = []
        
        for vac_id, vac_data in vacancies.items():
            result = self.calculate_score(
                cv_id=cv_id,
                cv_text=cv_data['text'],
                cv_skills=cv_data['skills'],
                vacancy_id=vac_id,
                vacancy_title=vac_data['title'],
                vacancy_text=vac_data['description'],
                vacancy_skills=vac_data['skills'],
                corpus=corpus
            )
            results.append(result)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é total_score
        results.sort(key=lambda x: x.total_score, reverse=True)
        
        return results
    
    def get_ranking_array(self, results: List[MatchResult]) -> List[int]:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Å—Å–∏–≤ —Ä–∞–Ω–≥–æ–≤ (1-5) –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å ground truth
        
        1 = –ª—É—á—à–∞—è –≤–∞–∫–∞–Ω—Å–∏—è (–Ω–∞–∏–≤—ã—Å—à–∏–π score)
        5 = —Ö—É–¥—à–∞—è –≤–∞–∫–∞–Ω—Å–∏—è (–Ω–∞–∏–º–µ–Ω—å—à–∏–π score)
        """
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é total_score
        sorted_results = sorted(results, key=lambda x: x.total_score, reverse=True)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤ —Ä–∞–Ω–≥–æ–≤ –¥–ª—è vacancy_id 1-5
        ranks = [0] * 5
        
        for position, result in enumerate(sorted_results, 1):
            # vacancy_id —É–∂–µ 1-5
            vacancy_index = result.vacancy_id - 1
            ranks[vacancy_index] = position
        
        return ranks
    
    def _update_stats(self, result: MatchResult):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.stats['total_matches'] += 1
        n = self.stats['total_matches']
        
        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        self.stats['avg_keyword_score'] += (result.keyword_score - self.stats['avg_keyword_score']) / n
        self.stats['avg_tfidf_score'] += (result.tfidf_score - self.stats['avg_tfidf_score']) / n
        self.stats['avg_semantic_score'] += (result.semantic_score - self.stats['avg_semantic_score']) / n
        self.stats['avg_total_score'] += (result.total_score - self.stats['avg_total_score']) / n
    
    def print_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê UNIFIED SCORER")
        print("=" * 50)
        print(f"–í—Å–µ–≥–æ –º–∞—Ç—á–µ–π: {self.stats['total_matches']}")
        print(f"–°—Ä–µ–¥–Ω–∏–π Keyword score: {self.stats['avg_keyword_score']:.1f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π TF-IDF score: {self.stats['avg_tfidf_score']:.1f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π Semantic score: {self.stats['avg_semantic_score']:.1f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π Total score: {self.stats['avg_total_score']:.1f}")
        print("=" * 50)


class VMMethodAdapter:
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è Vector Matching –º–µ—Ç–æ–¥–∞ 
    """
    
    def __init__(self):
        print("‚úÖ VM Method Adapter initialized")
    
    def summarize_text(self, text: str, max_sentences: int = 10) -> str:
        """
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è VM –º–µ—Ç–æ–¥–∞ –∏–∑ —Å—Ç–∞—Ç—å–∏
        """
        if not text:
            return ""
        
        import re
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if len(sentences) <= max_sentences:
            return text[:2000]
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        summary = '. '.join(sentences[:max_sentences]) + '.'
        return summary[:2000]
    
    def get_char_ngrams(self, text: str, n_range: Tuple[int, int] = (1, 3)):
        """
        Character n-gram –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        """
        from sklearn.feature_extraction.text import CountVectorizer
        
        if len(text) > 5000:
            text = text[:5000]
        
        try:
            vectorizer = CountVectorizer(
                analyzer='char',
                ngram_range=n_range,
                lowercase=True,
                max_features=2000
            )
            vec = vectorizer.fit_transform([text])
            return vec
        except Exception as e:
            print(f"‚ö†Ô∏è Vectorization error: {e}")
            from scipy.sparse import csr_matrix
            return csr_matrix((1, 2000))
    
    def l1_distance(self, vec1, vec2) -> float:
        """
        L1 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Å–ø–∞—Ä—Å-–≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        try:
            from scipy.spatial.distance import cityblock
            
            if hasattr(vec1, 'toarray'):
                vec1 = vec1.toarray().flatten()
            if hasattr(vec2, 'toarray'):
                vec2 = vec2.toarray().flatten()
            
            min_len = min(len(vec1), len(vec2))
            vec1 = vec1[:min_len]
            vec2 = vec2[:min_len]
            
            return cityblock(vec1, vec2)
        except Exception as e:
            print(f"‚ö†Ô∏è Distance error: {e}")
            return 1000000.0
    
    def get_vector_fixed(self, text: str, max_features: int = 1000) -> np.ndarray:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –§–ò–ö–°–ò–†–û–í–ê–ù–ù–û–ô —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
        try:
            vec = vectorizer.fit_transform([text]).toarray()[0]
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            if np.linalg.norm(vec) > 0:
                vec = vec / np.linalg.norm(vec)
            return vec
        except:
            return np.zeros(max_features)
    
    def l1_distance_fixed(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        L1 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –û–î–ò–ù–ê–ö–û–í–û–ô —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """
        from scipy.spatial.distance import cityblock
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã
        assert len(vec1) == len(vec2), f"Vector dimensions mismatch: {len(vec1)} vs {len(vec2)}"
        return cityblock(vec1, vec2)