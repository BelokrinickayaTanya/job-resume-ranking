"""
–ú–æ–¥—É–ª—å –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏:
- Krippendorff's Alpha
- Spearman's Rank Correlation
- Cohen's Kappa
- Accuracy (–¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
"""
from typing import List, Dict, Tuple, Optional, Union
import numpy as np
from scipy.stats import spearmanr, kendalltau
from scipy.spatial.distance import pdist, squareform
from sklearn.metrics import cohen_kappa_score, accuracy_score
import pandas as pd
from collections import Counter


class RankingMetrics:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    
    @staticmethod
    def krippendorff_alpha(
        data: Union[List[List[int]], np.ndarray],
        level: str = 'ordinal'
    ) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Krippendorff's Alpha - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
        """
        data = np.array(data)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if data.ndim != 2:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 2-–º–µ—Ä–Ω—ã–º –º–∞—Å—Å–∏–≤–æ–º")
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–æ–∂–∏–¥–∞–µ–º raters √ó items)
        if data.shape[0] > data.shape[1]:
            data = data.T
        
        n_raters, n_items = data.shape
        
        # –ó–Ω–∞—á–µ–Ω–∏—è –∏ —á–∞—Å—Ç–æ—Ç—ã
        values = np.unique(data[~np.isnan(data)])
        n_values = len(values)
        
        if n_values == 0:
            return 1.0
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        o = np.zeros((n_values, n_values))
        
        for i in range(n_items):
            column = data[:, i]
            column = column[~np.isnan(column)]
            for val1 in column:
                for val2 in column:
                    idx1 = np.where(values == val1)[0][0]
                    idx2 = np.where(values == val2)[0][0]
                    o[idx1, idx2] += 1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        o = o / np.sum(o) if np.sum(o) > 0 else o
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        col_sum = np.sum(o, axis=0)
        row_sum = np.sum(o, axis=1)
        e = np.outer(row_sum, col_sum)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–ª—è ordinal –¥–∞–Ω–Ω—ã—Ö
        d = np.zeros((n_values, n_values))
        if level == 'ordinal':
            for i in range(n_values):
                for j in range(n_values):
                    d[i, j] = (i - j) ** 2  # –ö–≤–∞–¥—Ä–∞—Ç —Ä–∞–∑–Ω–∏—Ü—ã —Ä–∞–Ω–≥–æ–≤
        
        # –†–∞—Å—á–µ—Ç alpha
        observed_agreement = np.sum(o * d)
        expected_agreement = np.sum(e * d)
        
        if expected_agreement == 0:
            return 1.0
        
        alpha = 1 - (observed_agreement / expected_agreement)
        
        return round(alpha, 4)

    
    @staticmethod
    def spearman_correlation(
        ranking1: List[int],
        ranking2: List[int]
    ) -> Dict[str, float]:
        """
        Spearman's rank correlation coefficient
        
        Args:
            ranking1: –ü–µ—Ä–≤—ã–π –º–∞—Å—Å–∏–≤ —Ä–∞–Ω–≥–æ–≤ (1-5)
            ranking2: –í—Ç–æ—Ä–æ–π –º–∞—Å—Å–∏–≤ —Ä–∞–Ω–≥–æ–≤ (1-5)
        
        Returns:
            {'rho': float, 'p_value': float}
        """
        rho, p_value = spearmanr(ranking1, ranking2)
        
        return {
            'rho': round(rho, 4),
            'p_value': round(p_value, 4)
        }
    
    @staticmethod
    def cohen_kappa(
        ranking1: List[int],
        ranking2: List[int],
        weights: Optional[str] = 'quadratic'
    ) -> float:
        """
        Cohen's kappa coefficient
        
        Args:
            ranking1: –ü–µ—Ä–≤—ã–π –º–∞—Å—Å–∏–≤ —Ä–∞–Ω–≥–æ–≤
            ranking2: –í—Ç–æ—Ä–æ–π –º–∞—Å—Å–∏–≤ —Ä–∞–Ω–≥–æ–≤
            weights: 'linear', 'quadratic', None
        """
        kappa = cohen_kappa_score(ranking1, ranking2, weights=weights)
        return round(kappa, 4)
    
    @staticmethod
    def accuracy_at_k(
        predicted_rankings: List[List[int]],
        true_rankings: List[List[int]],
        k: int = 1
    ) -> float:
        """
        Accuracy@k - —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è top-k –≤–∞–∫–∞–Ω—Å–∏–π
        
        Args:
            predicted_rankings: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            true_rankings: –ò—Å—Ç–∏–Ω–Ω—ã–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            k: –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å top-k –≤–∞–∫–∞–Ω—Å–∏–π
        """
        correct = 0
        total = len(predicted_rankings)
        
        for pred, true in zip(predicted_rankings, true_rankings):
            # –ò–Ω–¥–µ–∫—Å—ã top-k –≤–∞–∫–∞–Ω—Å–∏–π
            pred_topk = set(np.argsort(pred)[:k])
            true_topk = set(np.argsort(true)[:k])
            
            if pred_topk == true_topk:
                correct += 1
        
        return round(correct / total, 4)
    
    @staticmethod
    def ndcg_score(
        predicted_rankings: List[int],
        true_rankings: List[int],
        k: int = 5
    ) -> float:
        """
        Normalized Discounted Cumulative Gain @k
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞–Ω–≥–∏ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (1-5, 5 - –ª—É—á—à–∞—è)
        true_relevance = [6 - r for r in true_rankings]
        pred_relevance = [6 - r for r in predicted_rankings]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é
        sorted_indices = np.argsort(pred_relevance)[::-1]
        
        dcg = 0
        idcg = 0
        
        for i, idx in enumerate(sorted_indices[:k]):
            dcg += (2 ** true_relevance[idx] - 1) / np.log2(i + 2)
        
        # –ò–¥–µ–∞–ª—å–Ω—ã–π DCG
        ideal_relevance = sorted(true_relevance, reverse=True)
        for i in range(min(k, len(ideal_relevance))):
            idcg += (2 ** ideal_relevance[i] - 1) / np.log2(i + 2)
        
        if idcg == 0:
            return 0.0
        
        return round(dcg / idcg, 4)
    
    @staticmethod
    def mean_reciprocal_rank(
        predicted_rankings: List[List[int]],
        true_rankings: List[List[int]]
    ) -> float:
        """
        Mean Reciprocal Rank - —Å—Ä–µ–¥–Ω–∏–π –æ–±—Ä–∞—Ç–Ω—ã–π —Ä–∞–Ω–≥ –ø–µ—Ä–≤–æ–π –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
        """
        mrrs = []
        
        for pred, true in zip(predicted_rankings, true_rankings):
            # –ò–Ω–¥–µ–∫—Å –ª—É—á—à–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ ground truth
            best_vacancy = np.argmin(true)
            
            # –†–∞–Ω–≥ —ç—Ç–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
            rank = pred[best_vacancy]
            
            mrrs.append(1.0 / rank)
        
        return round(np.mean(mrrs), 4)
    
    @staticmethod
    def pairwise_accuracy(
        predicted_rankings: List[int],
        true_rankings: List[int]
    ) -> float:
        """
        Pairwise accuracy - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä
        """
        n = len(predicted_rankings)
        correct_pairs = 0
        total_pairs = 0
        
        for i in range(n):
            for j in range(i + 1, n):
                total_pairs += 1
                
                pred_order = predicted_rankings[i] < predicted_rankings[j]
                true_order = true_rankings[i] < true_rankings[j]
                
                if pred_order == true_order:
                    correct_pairs += 1
        
        return round(correct_pairs / total_pairs, 4)


class Evaluator:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    
    def __init__(self, ground_truth: Dict[int, List[int]]):
        """
        Args:
            ground_truth: –°–ª–æ–≤–∞—Ä—å {cv_id: [rank_vac1, ..., rank_vac5]}
        """
        self.ground_truth = ground_truth
        self.metrics = RankingMetrics()
        self.results_history = []
    
    def evaluate(
        self,
        predictions: Dict[int, List[int]],
        method_name: str
    ) -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∑—é–º–µ 
        """
        cv_ids = list(self.ground_truth.keys())
        
        pred_list = []
        true_list = []
        
        for cv_id in cv_ids:
            if cv_id in predictions:
                pred_list.append(predictions[cv_id])
                true_list.append(self.ground_truth[cv_id])
        
        if not pred_list:
            return {}
        
        # 1. Krippendorff's Alpha - –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –æ—Ç–¥–µ–ª—å–Ω–æ
        alpha_values = []
        for pred, true in zip(pred_list, true_list):
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É 2 raters √ó 5 items
            alpha_data = [pred, true]
            alpha = self.metrics.krippendorff_alpha(alpha_data, level='ordinal')
            alpha_values.append(alpha)
        
        avg_alpha = np.mean(alpha_values) if alpha_values else 0.0
        
        # 2. Spearman correlation
        spearman_values = []
        for pred, true in zip(pred_list, true_list):
            rho, _ = spearmanr(pred, true)
            if not np.isnan(rho):
                spearman_values.append(rho)
        avg_spearman = np.mean(spearman_values) if spearman_values else 0.0
        
        # 3. Cohen's Kappa
        kappa_values = []
        for pred, true in zip(pred_list, true_list):
            kappa = cohen_kappa_score(pred, true, weights='quadratic')
            if not np.isnan(kappa):
                kappa_values.append(kappa)
        avg_kappa = np.mean(kappa_values) if kappa_values else 0.0
        
        # 4. Accuracy@1
        acc_at_1 = self.metrics.accuracy_at_k(pred_list, true_list, k=1)
        
        # 5. NDCG@5
        ndcg_values = []
        for pred, true in zip(pred_list, true_list):
            ndcg = self.metrics.ndcg_score(pred, true, k=5)
            if not np.isnan(ndcg):
                ndcg_values.append(ndcg)
        avg_ndcg = np.mean(ndcg_values) if ndcg_values else 0.0
        
        # 6. MRR
        mrr = self.metrics.mean_reciprocal_rank(pred_list, true_list)
        
        results = {
            'method': method_name,
            'krippendorff_alpha': round(avg_alpha, 4),
            'spearman_rho': round(avg_spearman, 4),
            'cohen_kappa': round(avg_kappa, 4),
            'accuracy_at_1': round(acc_at_1, 4),
            'accuracy_at_3': round(self.metrics.accuracy_at_k(pred_list, true_list, k=3), 4),
            'ndcg@5': round(avg_ndcg, 4),
            'mrr': round(mrr, 4),
            'num_samples': len(pred_list)
        }
        
        self.results_history.append(results)
        return results
    
    def evaluate_bidirectional(
        self,
        predictions: Dict[int, List[int]],
        method_name: str
    ) -> Dict[str, float]:
        """
        –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            predictions: –°–ª–æ–≤–∞—Ä—å {cv_id: [rank_vac1, ..., rank_vac5]}
            method_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞
        
        Returns:
            Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π
        """
        # 1. –ü—Ä—è–º–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (–≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ CV) - –∫–∞–∫ —Å–µ–π—á–∞—Å
        cv_scores = self.evaluate(predictions, f"{method_name}_cv")
        
        # 2. –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (CV –¥–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏)
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        vac_predictions = {}
        vac_ground_truth = {}
        
        # ID –≤–∞–∫–∞–Ω—Å–∏–π 1-5
        for vac_id in range(1, 6):
            vac_predictions[vac_id] = []
            vac_ground_truth[vac_id] = []
            
            # –í—Å–µ CV —Å 1 –ø–æ 30
            for cv_id in range(1, 31):
                if cv_id in predictions and cv_id in self.ground_truth:
                    # –†–∞–Ω–≥ —ç—Ç–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–ª—è CV
                    pred_rank = predictions[cv_id][vac_id - 1]
                    vac_predictions[vac_id].append(pred_rank)
                    
                    # –†–∞–Ω–≥ —ç—Ç–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ ground truth –¥–ª—è CV
                    true_rank = self.ground_truth[cv_id][vac_id - 1]
                    vac_ground_truth[vac_id].append(true_rank)
        
        # 3. –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
        vac_alphas = []
        vac_spearmans = []
        vac_acc1s = []
        
        for vac_id in range(1, 6):
            if vac_id in vac_predictions and vac_id in vac_ground_truth:
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è evaluate
                temp_pred = {vac_id: vac_predictions[vac_id]}
                temp_gt = {vac_id: vac_ground_truth[vac_id]}
                
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π evaluator –¥–ª—è —ç—Ç–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
                temp_evaluator = Evaluator(temp_gt)
                vac_scores = temp_evaluator.evaluate(temp_pred, f"{method_name}_vac{vac_id}")
                
                vac_alphas.append(vac_scores['krippendorff_alpha'])
                vac_spearmans.append(vac_scores['spearman_rho'])
                vac_acc1s.append(vac_scores['accuracy_at_1'])
        
        # 4. –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        vac_scores_avg = {
            'krippendorff_alpha': np.mean(vac_alphas) if vac_alphas else 0.0,
            'spearman_rho': np.mean(vac_spearmans) if vac_spearmans else 0.0,
            'accuracy_at_1': np.mean(vac_acc1s) if vac_acc1s else 0.0
        }
        
        # 5. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        combined_alpha = (cv_scores['krippendorff_alpha'] + vac_scores_avg['krippendorff_alpha']) / 2
        combined_spearman = (cv_scores['spearman_rho'] + vac_scores_avg['spearman_rho']) / 2
        combined_acc1 = (cv_scores['accuracy_at_1'] + vac_scores_avg['accuracy_at_1']) / 2
        
        # 6. –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results = {
            'method': f"{method_name}_bidirectional",
            'krippendorff_alpha': round(cv_scores['krippendorff_alpha'], 4),
            'spearman_rho': round(cv_scores['spearman_rho'], 4),
            'accuracy_at_1': round(cv_scores['accuracy_at_1'], 4),
            'ndcg@5': round(cv_scores.get('ndcg@5', 0), 4),
            'mrr': round(cv_scores.get('mrr', 0), 4),
            'cv_krippendorff_alpha': round(cv_scores['krippendorff_alpha'], 4),
            'cv_spearman_rho': round(cv_scores['spearman_rho'], 4),
            'cv_accuracy_at_1': round(cv_scores['accuracy_at_1'], 4),
            'cv_ndcg@5': round(cv_scores.get('ndcg@5', 0), 4),
            'cv_mrr': round(cv_scores.get('mrr', 0), 4),
            'vac_krippendorff_alpha': round(vac_scores_avg['krippendorff_alpha'], 4),
            'vac_spearman_rho': round(vac_scores_avg['spearman_rho'], 4),
            'vac_accuracy_at_1': round(vac_scores_avg['accuracy_at_1'], 4),
            'combined_krippendorff_alpha': round(combined_alpha, 4),
            'combined_spearman_rho': round(combined_spearman, 4),
            'combined_accuracy_at_1': round(combined_acc1, 4)
        }
        
        self.results_history.append(results)
        return results

    def compare_methods(self) -> pd.DataFrame:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Ü–µ–Ω–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
        """
        if not self.results_history:
            print("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return pd.DataFrame()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: —É bidirectional –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏
        normalized_results = []
        for res in self.results_history:
            if 'combined_krippendorff_alpha' in res:
                # –≠—Ç–æ bidirectional —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                normalized = {
                    'method': res['method'],
                    'krippendorff_alpha': res['combined_krippendorff_alpha'],
                    'spearman_rho': res['combined_spearman_rho'],
                    'accuracy_at_1': res['combined_accuracy_at_1'],
                    'ndcg@5': res.get('ndcg@5', 0),
                    'mrr': res.get('mrr', 0)
                }
                normalized_results.append(normalized)
            else:
                normalized_results.append(res)
        
        df = pd.DataFrame(normalized_results)
        
        if 'krippendorff_alpha' in df.columns:
            df = df.sort_values('krippendorff_alpha', ascending=False)
        
        return df
    
    def print_comparison(self):
        """
        –í—ã–≤–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å
        """
        df = self.compare_methods()
        
        print("\n" + "=" * 80)
        print("–°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø".center(80))
        print("=" * 80)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        columns = ['method', 'krippendorff_alpha', 'spearman_rho', 
                  'accuracy_at_1', 'ndcg@5', 'mrr']
        
        print(f"\n{'–ú–µ—Ç–æ–¥':<30} {'Alpha':<10} {'Spearman':<10} {'Acc@1':<10} {'NDCG@5':<10} {'MRR':<10}")
        print("-" * 80)
        
        for _, row in df.iterrows():
            print(f"{row['method'][:30]:<30} "
                  f"{row['krippendorff_alpha']:<10.4f} "
                  f"{row['spearman_rho']:<10.4f} "
                  f"{row['accuracy_at_1']:<10.4f} "
                  f"{row['ndcg@5']:<10.4f} "
                  f"{row['mrr']:<10.4f}")
        
        print("=" * 80)
        
        # –õ—É—á—à–∏–π –º–µ—Ç–æ–¥
        best = df.iloc[0]
        print(f"\nüèÜ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best['method']}")
        print(f"   Krippendorff's Alpha: {best['krippendorff_alpha']:.4f}")
        print(f"   Spearman's Rho: {best['spearman_rho']:.4f}")
        print(f"   Accuracy@1: {best['accuracy_at_1']:.4f}")